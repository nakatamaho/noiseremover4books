{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease   \n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Fetched 336 kB in 2s (177 kB/s)  \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Calculating upgrade... Done\n",
      "The following packages have been kept back:\n",
      "  libnvinfer-plugin7 libnvinfer7\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libgl1-mesa-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libglib2.0-0 is already the newest version (2.64.6-1~ubuntu20.04.4).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get upgrade -y\n",
    "!apt install -y libgl1-mesa-dev\n",
    "!apt install -y libglib2.0-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.17.3; python_version >= \"3.8\" in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.23.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import ImageFile\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import os\n",
    "import shutil\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14 for GeForce RTX 3080 10GB\n",
    "#55 for V100 40GB\n",
    "images_per_batch = 14\n",
    "window_width = 256\n",
    "window_height = 256\n",
    "model_name=\"00/scannoise_remover\"\n",
    "make_noised_images = \"yes\"\n",
    "make_image_split = \"yes\"\n",
    "noise_algorithm = \"new\"\n",
    "groundtruthdir=\"/work/groundtruth/\"\n",
    "cleanimagedir=\"/work/clean/\"\n",
    "noisedimagedir=\"/work/noised/\"\n",
    "epochs = 100\n",
    "images_per_bulk = images_per_batch * 100\n",
    "validation_percent = 20\n",
    "restart_epoch = 0\n",
    "testimages_max = 3000\n",
    "if restart_epoch != 0:\n",
    "    restart = \"yes\"\n",
    "else:\n",
    "    restart = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_linenoise(filename, out_dirname):\n",
    "    sigma = 0.35\n",
    "    max_thickness = 26\n",
    "    min_thickness = 10\n",
    "\n",
    "    image = cv2.imread(filename)\n",
    "    height, width, c = image.shape\n",
    "    output_filename_woext = os.path.splitext(os.path.basename(filename))[0]\n",
    "    output_dir = os.path.split(os.path.dirname(filename))\n",
    "    os.makedirs(out_dirname +\"/\" + output_dir[-1], exist_ok=True)\n",
    "\n",
    "    randline_img = image.copy()\n",
    "    randline_img = np.array(randline_img, dtype='float32')\n",
    "    orig_img = randline_img.copy()\n",
    "\n",
    "    thickness=random.randint(1,max_thickness/2)*2+1\n",
    "    rand_x=random.randint(      thickness - int(thickness/2) - 1, width - int(thickness/2) - 1)\n",
    "\n",
    "    #print(\"noise center\", rand_x)\n",
    "    #print(\"noise thickness\", thickness)\n",
    "\n",
    "    __col = random.randint(0,7)\n",
    "    _r = __col % 2\n",
    "    _g = (__col >> 1) % 2\n",
    "    _b = (__col >> 2) % 2\n",
    "    _flag = random.randrange(-1,2,2)\n",
    "    center_high = 18\n",
    "    center_low = 6\n",
    "    variance_high = 3\n",
    "    variance_low = 1\n",
    "    weight_center = random.randrange(center_low,center_high)/100.\n",
    "    weight_width = random.randrange(variance_low,variance_high)/100.\n",
    "\n",
    "    while True:\n",
    "        for __x in range(thickness):\n",
    "            _x = __x - int(thickness/2)\n",
    "            distance_from_center = - (int(thickness/2) - abs(_x) ) / int(thickness/2) + 1\n",
    "            gauss_dist = math.exp ( - (distance_from_center) **2  / (2.0 * sigma **2 ) )\n",
    "            for _y in range(height):\n",
    "                _col = randline_img[_y, rand_x + _x]\n",
    "                weight = random.triangular(weight_center-weight_width, weight_center+weight_width\n",
    "                                                                        , weight_center)\n",
    "                _col[0] = (_r * 255.) * gauss_dist * _flag * weight + _col [0]\n",
    "                _col[1] = (_g * 255.) * gauss_dist * _flag * weight + _col [1]\n",
    "                _col[2] = (_b * 255.) * gauss_dist * _flag * weight + _col [2]\n",
    "                if _col[0] > 255.:\n",
    "                    _col[0] = 255.\n",
    "                if _col[1] > 255.:\n",
    "                    _col[1] = 255.\n",
    "                if _col[2] > 255.:\n",
    "                    _col[2] = 255.\n",
    "                if _col[0] < 0.:\n",
    "                    _col[0] = 0.\n",
    "                if _col[1] < 0.:\n",
    "                    _col[1] = 0.\n",
    "                if _col[2] < 0.:\n",
    "                    _col[2] = 0.\n",
    "                randline_img[_y, rand_x + _x] = _col\n",
    "        randline_img = np.array(randline_img, dtype='int16')\n",
    "        orig_img = np.array(orig_img, dtype='int16')\n",
    "        randline_img_rgb = randline_img[:,:,0]+randline_img[:,:,1]+randline_img[:,:,2]\n",
    "        orig_img_rgb = orig_img[:,:,0]+orig_img[:,:,1]+orig_img[:,:,2]\n",
    "\n",
    "        diff = np.abs(randline_img_rgb - orig_img_rgb)\n",
    "        diff_max = diff.max()\n",
    "        diff_sum = diff.sum()\n",
    "        if diff_sum < 1200 or diff_max < 10:\n",
    "            #print(output_filename_woext, np.abs(diff).max())\n",
    "            #print(\"no diff\")\n",
    "            __col = random.randint(0,7)\n",
    "            _r = __col % 2\n",
    "            _g = (__col >> 1) % 2\n",
    "            _b = (__col >> 2) % 2\n",
    "            _flag = random.randrange(-1,2,2)\n",
    "            weight_center = random.randrange(center_low,center_high)/100.\n",
    "            weight_width = random.randrange(variance_low,variance_high)/100.\n",
    "            randline_img = image.copy()\n",
    "            randline_img = np.array(randline_img, dtype='float32')\n",
    "            orig_img = randline_img.copy()\n",
    "            continue\n",
    "        else:\n",
    "            #print(\"absmax, sum, _flag, weight_center, weight_width, thickness\",  output_filename_woext, diff_max, diff_sum, _flag, weight_center, weight_width, thickness)\n",
    "            #print(\"ok\")\n",
    "            randline_img = np.array(randline_img, dtype='uint8')\n",
    "#            if _flag > 1:\n",
    "#                diff = np.array(orig_img - randline_img, dtype='uint8')\n",
    "#            else:\n",
    "#                diff = np.array(randline_img - orig_img, dtype='uint8')\n",
    "            cv2.imwrite(out_dirname +\"/\" + output_dir[-1] + \"/\" + output_filename_woext+\".png\",randline_img)\n",
    "#            cv2.imwrite(out_dirname +\"/\" + output_dir[-1] + \"/\" + output_filename_woext+\".diff.png\",diff)\n",
    "            break\n",
    "    return (filename, rand_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_linenoise_old(filename, out_dirname):\n",
    "    sigma = 0.35\n",
    "    max_thickness = 12\n",
    "    min_thickness = 3\n",
    "    image = cv2.imread(filename)\n",
    "    height, width, c = image.shape\n",
    "    \n",
    "    thickness=random.randint(1,max_thickness/2)*2+1\n",
    "    rand_x=random.randint(      thickness - int(thickness/2) - 1, width - int(thickness/2) - 1)\n",
    "    randline_img = image.copy()\n",
    "    randline_img = np.array(randline_img, dtype='float32')\n",
    "    \n",
    "    output_filename_woext = os.path.splitext(os.path.basename(filename))[0]\n",
    "    output_dir = os.path.split(os.path.dirname(filename))\n",
    "    os.makedirs(out_dirname +\"/\" + output_dir[-1], exist_ok=True) \n",
    "    \n",
    "    #print(\"noise center\", rand_x)\n",
    "    #print(\"noise thickness\", thickness)\n",
    "\n",
    "    __col = random.randint(0,7)\n",
    "    _r = __col % 2\n",
    "    _g = (__col >> 1) % 2\n",
    "    _b = (__col >> 2) % 2\n",
    "\n",
    "    for __x in range(thickness):\n",
    "        _x = __x - int(thickness/2)\n",
    "        distance_from_center = - (int(thickness/2) - abs(_x) ) / int(thickness/2) + 1\n",
    "        gauss_dist = math.exp ( - (distance_from_center) **2  / (2.0 * sigma **2 ) )\n",
    "        for _y in range(height):\n",
    "            _col = randline_img[_y, rand_x + _x]\n",
    "            weight = random.triangular(0.1, 1.0, 0.3)\n",
    "            _col[0] = (_r * 255) * gauss_dist * weight + _col [0]\n",
    "            _col[1] = (_g * 255) * gauss_dist * weight + _col [1]\n",
    "            _col[2] = (_b * 255) * gauss_dist * weight + _col [2]\n",
    "            if _col[0] > 255:\n",
    "                _col[0] = 255\n",
    "            if _col[1] > 255:\n",
    "                _col[1] = 255\n",
    "            if _col[2] > 255:\n",
    "                _col[2] = 255\n",
    "            if _col[0] < 0:\n",
    "                _col[0] = 0\n",
    "            if _col[1] < 0:\n",
    "                _col[1] = 0\n",
    "            if _col[2] < 0:\n",
    "                _col[2] = 0\n",
    "            randline_img[_y, rand_x + _x] = _col\n",
    "    cv2.imwrite(out_dirname +\"/\" + output_dir[-1] + \"/\" + output_filename_woext+\".png\",randline_img)\n",
    "    return (filename, rand_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImgSplit(filename, out_dirname):\n",
    "    height = window_height\n",
    "    width = window_width\n",
    "    \n",
    "    im = cv2.imread(filename)\n",
    "    output_filename_woext = os.path.splitext(os.path.basename(filename))[0]\n",
    "    output_dir = os.path.split(os.path.dirname(filename))\n",
    "    os.makedirs(out_dirname +\"/\" + output_dir[-1], exist_ok=True) \n",
    "    img_height, img_width = im.shape[:2]\n",
    "    # do not make a patch when image patch is too small\n",
    "    if (img_height < window_height) or (img_width < window_width):\n",
    "        return\n",
    "    split_height = int(img_height / height) \n",
    "    split_width = int(img_width / width)\n",
    "    for h1 in range(split_height+1):\n",
    "        for w1 in range(split_width+1):\n",
    "            w2 = w1 * width\n",
    "            h2 = h1 * height\n",
    "            _h_start = h2\n",
    "            _h_end   = height+h2-1\n",
    "            _w_start = w2\n",
    "            _w_end   = width+w2-1\n",
    "            if h1 == split_height:\n",
    "                _h_start = img_height-height\n",
    "                _h_end   = img_height-1\n",
    "            if w1 == split_width:\n",
    "                _w_start = img_width-width\n",
    "                _w_end   = img_width-1\n",
    "            c = im[_h_start:_h_end+1, _w_start:_w_end+1]\n",
    "            if (_h_end+1 - _h_start) == height and (_w_end+1 - _w_start) == width:\n",
    "                cv2.imwrite(out_dirname +\"/\" + output_dir[-1] + \"/\" + output_filename_woext + \"_\" + str(h1).zfill(3) + \"_\" + str(w1).zfill(3) +\".png\",c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_image_split != \"no\":\n",
    "    print(\"making testimages\")\n",
    "    filenames_list=glob.glob(groundtruthdir \" + \"/A/*.jpg')\n",
    "    try:\n",
    "        shutil.rmtree(cleanimagedir)\n",
    "    except OSError as e:\n",
    "        pass    \n",
    "    pathlib.Path(cleanimagedir).mkdir()\n",
    "    \n",
    "    for _filename in tqdm.tqdm(filenames_list):\n",
    "        ImgSplit(_filename, out_dirname = cleanimagedir)\n",
    "else:\n",
    "    print(\"do not make testimages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if make_noised_images != \"no\":\n",
    "    print(\"making noised images\")\n",
    "    try:\n",
    "        shutil.rmtree(noisedimagedir)       \n",
    "    except OSError as e:\n",
    "        pass\n",
    "    pathlib.Path(noisedimagedir).mkdir()\n",
    "    filenames_list=glob.glob(cleanimagedir + \"*/*.png')\n",
    "    for _filename in tqdm.tqdm(filenames_list):\n",
    "        if noise_algorithm == \"old\":\n",
    "            #print(\"using old noise algorithm\")\n",
    "            pp = add_linenoise_old(_filename, out_dirname = noisedimagedir)\n",
    "        else:\n",
    "            #print(\"using new noise algorithm\")\n",
    "            pp = add_linenoise(_filename, out_dirname = noisedimagedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_train_list=sorted(glob.glob(noisedimagedir + \"*/*.png'))\n",
    "filenames_target_list=sorted(glob.glob(cleanimagedir + \"*/*.png'))\n",
    "counter_train = len(filenames_train_list)\n",
    "counter_target = len(filenames_target_list)\n",
    "if (counter_train != counter_target):\n",
    "    print(\"something wrong in num of pics of target and trains\")\n",
    "    exit(1)\n",
    "numof_train = int(counter_target * (100.0-validation_percent) / 100.0 )\n",
    "numof_validation = counter_target - numof_train\n",
    "print(\"total images\",counter_target)\n",
    "print(\"training images: \", numof_train)\n",
    "print(\"validation images: \", numof_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(_X_train, _Y_train, _images_per_batch):\n",
    "    print(\"getting a batch\")\n",
    "    print(\"images per batch\", _images_per_batch)\n",
    "    numof_batchs = len(_X_train)//_images_per_batch\n",
    "    print(\"Number of batches\", numof_batchs)\n",
    "\n",
    "    i = 0\n",
    "    while (i < numof_batchs):\n",
    "        print(\"doing a batch of \", i, \"/\", numof_batchs)\n",
    "        X_batch = []\n",
    "        Y_batch = []\n",
    "        Y_batch = _Y_train[(i * _images_per_batch):(i * _images_per_batch + _images_per_batch)]\n",
    "        X_batch = _X_train[(i * _images_per_batch):(i * _images_per_batch + _images_per_batch)]\n",
    "        i += 1\n",
    "        yield X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bulk(_X_train, _Y_train, _images_per_bulk):\n",
    "    print(\"images per bluk\", _images_per_bulk)\n",
    "    numof_bulks = len(_X_train)// _images_per_bulk\n",
    "    print(\"Number of bulks\", numof_bulks)\n",
    "    i = 0\n",
    "    while (i < numof_bulks):\n",
    "        print(\"doing a bulk of \", i, \"/\", numof_bulks)\n",
    "        X_train_bulk = []\n",
    "        Y_train_bulk = []\n",
    "        X_train_bulk_name = []\n",
    "        Y_train_bulk_name = []\n",
    "        print(\"getting a bluk...start \")\n",
    "        X_train_name = _X_train[(i * _images_per_bulk):(i * _images_per_bulk + _images_per_bulk)]\n",
    "        Y_train_name = _Y_train[(i * _images_per_bulk):(i * _images_per_bulk + _images_per_bulk)]\n",
    "        X_train_bulk = np.array([np.array(Image.open(file)) for file in X_train_name])\n",
    "        Y_train_bulk = np.array([np.array(Image.open(file)) for file in Y_train_name])\n",
    "        print(\"getting a bluk...done \")\n",
    "\n",
    "        i += 1\n",
    "        yield X_train_bulk, Y_train_bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split noised data to training and test (validation) sets\n",
    "X_train, x_test= train_test_split(filenames_train_list, test_size=validation_percent, random_state=42) #from sklearn\n",
    "Y_train = [s.replace(noisedimagedir, cleanimagedir) for s in X_train]\n",
    "y_test = [s.replace(noisedimagedir, cleanimagedir) for s in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(window_height, window_width, 3))\n",
    "x = Conv2D(64, (3, 3), padding='same')(input_img)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "for i in range(15):\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "x = Conv2D(3, (3, 3), padding='same')(x)\n",
    "output_img = Activation('tanh')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_img, output_img)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "if restart == \"yes\":\n",
    "    print(\"restarting from epoch #\", restart_epoch)\n",
    "    model.load_weights(model_name + '.' + str(restart_epoch-1).zfill(3) +'.h5')\n",
    "    \n",
    "for epoch in range(restart_epoch, epochs):\n",
    "    print(\"=\" * 80)\n",
    "    print(epoch, \"/\", epochs, \" EPOCHS\")\n",
    "    acc = []\n",
    "\n",
    "    for X_train_bulk, Y_train_bulk in get_bulk(X_train, Y_train, images_per_bulk):\n",
    "        for X_batch, Y_batch in get_batch(X_train_bulk, Y_train_bulk, images_per_batch):\n",
    "            print('Training start for a batch')\n",
    "            # normalize data\n",
    "            X_batch = X_batch.astype('float32')\n",
    "            X_batch /= 255\n",
    "            Y_batch = Y_batch.astype('float32')\n",
    "            Y_batch /= 255\n",
    "            model.train_on_batch(X_batch, Y_batch)\n",
    "            score = model.evaluate(X_batch, Y_batch)\n",
    "            print(\"batch accuracy:\", score)\n",
    "            acc.append(score)#\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    print(\"Train accuracy (mean)\", np.mean(acc))\n",
    "    print(\"Train accuracy (max)\", np.max(acc))\n",
    "    model.save(model_name + '.' + str(epoch).zfill(3) +'.h5')\n",
    "\n",
    "    #with tf.device('/cpu:0'):\n",
    "    #    loss, acc = model.evaluate(X_test, Y_test, verbose=2)\n",
    "    #    print(\"Loss and accuracy\", loss, acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
