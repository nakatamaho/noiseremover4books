{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Fetched 336 kB in 1s (244 kB/s)                                   \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Calculating upgrade... Done\n",
      "The following packages have been kept back:\n",
      "  libnvinfer-plugin7 libnvinfer7\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libgl1-mesa-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libglib2.0-0 is already the newest version (2.64.6-1~ubuntu20.04.4).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get upgrade -y\n",
    "!apt install -y libgl1-mesa-dev\n",
    "!apt install -y libglib2.0-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.14.5; python_version >= \"3.7\" in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.23.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 11 19:55:50 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   33C    P8    17W / 340W |   8483MiB / 10240MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import ImageFile\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import os\n",
    "import shutil\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14 for GeForce RTX 3080 10GB\n",
    "#55 for V100 40GB\n",
    "images_per_batch = 14\n",
    "window_width = 256\n",
    "window_height = 256\n",
    "model_name=\"scannoise_remover0\"\n",
    "make_noised_images = \"no\"\n",
    "make_image_split = \"no\"\n",
    "noise_algorithm = \"new\"\n",
    "epochs = 100\n",
    "images_per_bulk = images_per_batch * 100\n",
    "validation_percent = 0.1\n",
    "restart_epoch = 0\n",
    "testimages_max = 3000\n",
    "if restart_epoch != 0:\n",
    "    restart = \"yes\"\n",
    "else:\n",
    "    restart = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_linenoise(filename, out_dirname):\n",
    "    sigma = 0.35\n",
    "    max_thickness = 26\n",
    "    min_thickness = 10\n",
    "\n",
    "    image = cv2.imread(filename)\n",
    "    height, width, c = image.shape\n",
    "    output_filename_woext = os.path.splitext(os.path.basename(filename))[0]\n",
    "    output_dir = os.path.split(os.path.dirname(filename))\n",
    "    os.makedirs(out_dirname +\"/\" + output_dir[-1], exist_ok=True)\n",
    "\n",
    "    randline_img = image.copy()\n",
    "    randline_img = np.array(randline_img, dtype='float32')\n",
    "    orig_img = randline_img.copy()\n",
    "\n",
    "    thickness=random.randint(1,max_thickness/2)*2+1\n",
    "    rand_x=random.randint(      thickness - int(thickness/2) - 1, width - int(thickness/2) - 1)\n",
    "\n",
    "    #print(\"noise center\", rand_x)\n",
    "    #print(\"noise thickness\", thickness)\n",
    "\n",
    "    __col = random.randint(0,7)\n",
    "    _r = __col % 2\n",
    "    _g = (__col >> 1) % 2\n",
    "    _b = (__col >> 2) % 2\n",
    "    _flag = random.randrange(-1,2,2)\n",
    "    center_high = 18\n",
    "    center_low = 6\n",
    "    variance_high = 3\n",
    "    variance_low = 1\n",
    "    weight_center = random.randrange(center_low,center_high)/100.\n",
    "    weight_width = random.randrange(variance_low,variance_high)/100.\n",
    "\n",
    "    while True:\n",
    "        for __x in range(thickness):\n",
    "            _x = __x - int(thickness/2)\n",
    "            distance_from_center = - (int(thickness/2) - abs(_x) ) / int(thickness/2) + 1\n",
    "            gauss_dist = math.exp ( - (distance_from_center) **2  / (2.0 * sigma **2 ) )\n",
    "            for _y in range(height):\n",
    "                _col = randline_img[_y, rand_x + _x]\n",
    "                weight = random.triangular(weight_center-weight_width, weight_center+weight_width\n",
    "                                                                        , weight_center)\n",
    "                _col[0] = (_r * 255.) * gauss_dist * _flag * weight + _col [0]\n",
    "                _col[1] = (_g * 255.) * gauss_dist * _flag * weight + _col [1]\n",
    "                _col[2] = (_b * 255.) * gauss_dist * _flag * weight + _col [2]\n",
    "                if _col[0] > 255.:\n",
    "                    _col[0] = 255.\n",
    "                if _col[1] > 255.:\n",
    "                    _col[1] = 255.\n",
    "                if _col[2] > 255.:\n",
    "                    _col[2] = 255.\n",
    "                if _col[0] < 0.:\n",
    "                    _col[0] = 0.\n",
    "                if _col[1] < 0.:\n",
    "                    _col[1] = 0.\n",
    "                if _col[2] < 0.:\n",
    "                    _col[2] = 0.\n",
    "                randline_img[_y, rand_x + _x] = _col\n",
    "        randline_img = np.array(randline_img, dtype='int16')\n",
    "        orig_img = np.array(orig_img, dtype='int16')\n",
    "        randline_img_rgb = randline_img[:,:,0]+randline_img[:,:,1]+randline_img[:,:,2]\n",
    "        orig_img_rgb = orig_img[:,:,0]+orig_img[:,:,1]+orig_img[:,:,2]\n",
    "\n",
    "        diff = np.abs(randline_img_rgb - orig_img_rgb)\n",
    "        diff_max = diff.max()\n",
    "        diff_sum = diff.sum()\n",
    "        if diff_sum < 1200 or diff_max < 10:\n",
    "            #print(output_filename_woext, np.abs(diff).max())\n",
    "            #print(\"no diff\")\n",
    "            __col = random.randint(0,7)\n",
    "            _r = __col % 2\n",
    "            _g = (__col >> 1) % 2\n",
    "            _b = (__col >> 2) % 2\n",
    "            _flag = random.randrange(-1,2,2)\n",
    "            weight_center = random.randrange(center_low,center_high)/100.\n",
    "            weight_width = random.randrange(variance_low,variance_high)/100.\n",
    "            randline_img = image.copy()\n",
    "            randline_img = np.array(randline_img, dtype='float32')\n",
    "            orig_img = randline_img.copy()\n",
    "            continue\n",
    "        else:\n",
    "            #print(\"absmax, sum, _flag, weight_center, weight_width, thickness\",  output_filename_woext, diff_max, diff_sum, _flag, weight_center, weight_width, thickness)\n",
    "            #print(\"ok\")\n",
    "            randline_img = np.array(randline_img, dtype='uint8')\n",
    "#            if _flag > 1:\n",
    "#                diff = np.array(orig_img - randline_img, dtype='uint8')\n",
    "#            else:\n",
    "#                diff = np.array(randline_img - orig_img, dtype='uint8')\n",
    "            cv2.imwrite(out_dirname +\"/\" + output_dir[-1] + \"/\" + output_filename_woext+\".png\",randline_img)\n",
    "#            cv2.imwrite(out_dirname +\"/\" + output_dir[-1] + \"/\" + output_filename_woext+\".diff.png\",diff)\n",
    "            break\n",
    "    return (filename, rand_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_linenoise_old(filename, out_dirname):\n",
    "    sigma = 0.35\n",
    "    max_thickness = 12\n",
    "    min_thickness = 3\n",
    "    image = cv2.imread(filename)\n",
    "    height, width, c = image.shape\n",
    "    \n",
    "    thickness=random.randint(1,max_thickness/2)*2+1\n",
    "    rand_x=random.randint(      thickness - int(thickness/2) - 1, width - int(thickness/2) - 1)\n",
    "    randline_img = image.copy()\n",
    "    randline_img = np.array(randline_img, dtype='float32')\n",
    "    \n",
    "    output_filename_woext = os.path.splitext(os.path.basename(filename))[0]\n",
    "    output_dir = os.path.split(os.path.dirname(filename))\n",
    "    os.makedirs(out_dirname +\"/\" + output_dir[-1], exist_ok=True) \n",
    "    \n",
    "    #print(\"noise center\", rand_x)\n",
    "    #print(\"noise thickness\", thickness)\n",
    "\n",
    "    __col = random.randint(0,7)\n",
    "    _r = __col % 2\n",
    "    _g = (__col >> 1) % 2\n",
    "    _b = (__col >> 2) % 2\n",
    "\n",
    "    for __x in range(thickness):\n",
    "        _x = __x - int(thickness/2)\n",
    "        distance_from_center = - (int(thickness/2) - abs(_x) ) / int(thickness/2) + 1\n",
    "        gauss_dist = math.exp ( - (distance_from_center) **2  / (2.0 * sigma **2 ) )\n",
    "        for _y in range(height):\n",
    "            _col = randline_img[_y, rand_x + _x]\n",
    "            weight = random.triangular(0.1, 1.0, 0.3)\n",
    "            _col[0] = (_r * 255) * gauss_dist * weight + _col [0]\n",
    "            _col[1] = (_g * 255) * gauss_dist * weight + _col [1]\n",
    "            _col[2] = (_b * 255) * gauss_dist * weight + _col [2]\n",
    "            if _col[0] > 255:\n",
    "                _col[0] = 255\n",
    "            if _col[1] > 255:\n",
    "                _col[1] = 255\n",
    "            if _col[2] > 255:\n",
    "                _col[2] = 255\n",
    "            if _col[0] < 0:\n",
    "                _col[0] = 0\n",
    "            if _col[1] < 0:\n",
    "                _col[1] = 0\n",
    "            if _col[2] < 0:\n",
    "                _col[2] = 0\n",
    "            randline_img[_y, rand_x + _x] = _col\n",
    "    cv2.imwrite(out_dirname +\"/\" + output_dir[-1] + \"/\" + output_filename_woext+\".png\",randline_img)\n",
    "    return (filename, rand_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImgSplit(filename, out_dirname):\n",
    "    height = window_height\n",
    "    width = window_width\n",
    "    \n",
    "    im = cv2.imread(filename)\n",
    "    output_filename_woext = os.path.splitext(os.path.basename(filename))[0]\n",
    "    output_dir = os.path.split(os.path.dirname(filename))\n",
    "    os.makedirs(out_dirname +\"/\" + output_dir[-1], exist_ok=True) \n",
    "    img_height, img_width = im.shape[:2]\n",
    "    # do not make a patch when image patch is too small\n",
    "    if (img_height < window_height) or (img_width < window_width):\n",
    "        return\n",
    "    split_height = int(img_height / height) \n",
    "    split_width = int(img_width / width)\n",
    "    for h1 in range(split_height+1):\n",
    "        for w1 in range(split_width+1):\n",
    "            w2 = w1 * width\n",
    "            h2 = h1 * height\n",
    "            _h_start = h2\n",
    "            _h_end   = height+h2-1\n",
    "            _w_start = w2\n",
    "            _w_end   = width+w2-1\n",
    "            if h1 == split_height:\n",
    "                _h_start = img_height-height\n",
    "                _h_end   = img_height-1\n",
    "            if w1 == split_width:\n",
    "                _w_start = img_width-width\n",
    "                _w_end   = img_width-1\n",
    "            c = im[_h_start:_h_end+1, _w_start:_w_end+1]\n",
    "            if (_h_end+1 - _h_start) == height and (_w_end+1 - _w_start) == width:\n",
    "                cv2.imwrite(out_dirname +\"/\" + output_dir[-1] + \"/\" + output_filename_woext + \"_\" + str(h1).zfill(3) + \"_\" + str(w1).zfill(3) +\".png\",c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do not make testimages\n"
     ]
    }
   ],
   "source": [
    "if make_image_split != \"no\":\n",
    "    print(\"making testimages\")\n",
    "    filenames_list=glob.glob('bookscan_linefix/groundtruth/[A,B,C,D,E,F]/*.jpg')\n",
    "    try:\n",
    "        shutil.rmtree('clean')\n",
    "    except OSError as e:\n",
    "        pass    \n",
    "    pathlib.Path('clean').mkdir()\n",
    "    \n",
    "    for _filename in tqdm.tqdm(filenames_list):\n",
    "        ImgSplit(_filename, out_dirname = \"clean\")\n",
    "else:\n",
    "    print(\"do not make testimages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if make_noised_images != \"no\":\n",
    "    print(\"making noised images\")\n",
    "    try:\n",
    "        shutil.rmtree('noised')       \n",
    "    except OSError as e:\n",
    "        pass\n",
    "    pathlib.Path('noised').mkdir()\n",
    "    filenames_list=glob.glob('clean/*/*.png')\n",
    "    for _filename in tqdm.tqdm(filenames_list):\n",
    "        if noise_algorithm == \"old\":\n",
    "            #print(\"using old noise algorithm\")\n",
    "            pp = add_linenoise_old(_filename, out_dirname = \"noised\")\n",
    "        else:\n",
    "            #print(\"using new noise algorithm\")\n",
    "            pp = add_linenoise(_filename, out_dirname = \"noised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images 673596\n",
      "training images:  606236\n",
      "validation images:  67360\n"
     ]
    }
   ],
   "source": [
    "filenames_train_list=sorted(glob.glob('noised/*/*.png'))\n",
    "filenames_target_list=sorted(glob.glob('clean/*/*.png'))\n",
    "counter_train = len(filenames_train_list)\n",
    "counter_target = len(filenames_target_list)\n",
    "if (counter_train != counter_target):\n",
    "    print(\"something wrong in num of pics of target and trains\")\n",
    "    exit(1)\n",
    "numof_train = int(counter_target * (1.0-validation_percent))\n",
    "numof_validation = counter_target - numof_train\n",
    "print(\"total images\",counter_target)\n",
    "print(\"training images: \", numof_train)\n",
    "print(\"validation images: \", numof_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(_X_train, _Y_train, _images_per_batch):\n",
    "    print(\"getting a batch\")\n",
    "    print(\"images per batch\", _images_per_batch)\n",
    "    numof_batchs = len(_X_train)//_images_per_batch\n",
    "    print(\"Number of batches\", numof_batchs)\n",
    "\n",
    "    i = 0\n",
    "    while (i < numof_batchs):\n",
    "        print(\"doing a batch of \", i, \"/\", numof_batchs)\n",
    "        X_batch = []\n",
    "        Y_batch = []\n",
    "        Y_batch = _Y_train[(i * _images_per_batch):(i * _images_per_batch + _images_per_batch)]\n",
    "        X_batch = _X_train[(i * _images_per_batch):(i * _images_per_batch + _images_per_batch)]\n",
    "        i += 1\n",
    "        yield X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bulk(_X_train, _Y_train, _images_per_bulk):\n",
    "    print(\"images per bluk\", _images_per_bulk)\n",
    "    numof_bulks = len(_X_train)// _images_per_bulk\n",
    "    print(\"Number of bulks\", numof_bulks)\n",
    "    i = 0\n",
    "    while (i < numof_bulks):\n",
    "        print(\"doing a bulk of \", i, \"/\", numof_bulks)\n",
    "        X_train_bulk = []\n",
    "        Y_train_bulk = []\n",
    "        X_train_bulk_name = []\n",
    "        Y_train_bulk_name = []\n",
    "        print(\"getting a bluk...start \")\n",
    "        X_train_name = _X_train[(i * _images_per_bulk):(i * _images_per_bulk + _images_per_bulk)]\n",
    "        Y_train_name = _Y_train[(i * _images_per_bulk):(i * _images_per_bulk + _images_per_bulk)]\n",
    "        X_train_bulk = np.array([np.array(Image.open(file)) for file in X_train_name])\n",
    "        Y_train_bulk = np.array([np.array(Image.open(file)) for file in Y_train_name])\n",
    "        print(\"getting a bluk...done \")\n",
    "\n",
    "        i += 1\n",
    "        yield X_train_bulk, Y_train_bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split noised data to training and test (validation) sets\n",
    "X_train, x_test= train_test_split(filenames_train_list, test_size=validation_percent, random_state=42) #from sklearn\n",
    "Y_train = [s.replace('noised', 'clean') for s in X_train]\n",
    "y_test = [s.replace('noised', 'clean') for s in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(window_height, window_width, 3))\n",
    "x = Conv2D(64, (3, 3), padding='same')(input_img)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "for i in range(15):\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "x = Conv2D(3, (3, 3), padding='same')(x)\n",
    "output_img = Activation('tanh')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_img, output_img)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "0 / 100  EPOCHS\n",
      "images per bluk 1400\n",
      "Number of bulks 433\n",
      "doing a bulk of  0 / 433\n",
      "getting a bluk...start \n",
      "getting a bluk...done \n",
      "getting a batch\n",
      "images per batch 14\n",
      "Number of batches 100\n",
      "doing a batch of  0 / 100\n",
      "Training start for a batch\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/activation/Relu' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 378, in dispatch_queue\n      yield self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 250, in wrapper\n      runner = Runner(ctx_run, result, future, yielded)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 748, in __init__\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 540, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-26-2057914afe72>\", line 20, in <module>\n      model.train_on_batch(X_batch, Y_batch)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2478, in train_on_batch\n      logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/core/activation.py\", line 59, in call\n      return self.activation(inputs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/activations.py\", line 317, in relu\n      return backend.relu(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5369, in relu\n      x = tf.nn.relu(x)\nNode: 'model/activation/Relu'\nDNN library is not found.\n\t [[{{node model/activation/Relu}}]] [Op:__inference_train_function_6892]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m Y_batch \u001b[38;5;241m=\u001b[39m Y_batch\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m Y_batch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_batch, Y_batch)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, score)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:2478\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2474\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2475\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[1;32m   2476\u001b[0m     )\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[0;32m-> 2478\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2480\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/activation/Relu' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 378, in dispatch_queue\n      yield self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 250, in wrapper\n      runner = Runner(ctx_run, result, future, yielded)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 748, in __init__\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 540, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-26-2057914afe72>\", line 20, in <module>\n      model.train_on_batch(X_batch, Y_batch)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2478, in train_on_batch\n      logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/core/activation.py\", line 59, in call\n      return self.activation(inputs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/activations.py\", line 317, in relu\n      return backend.relu(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5369, in relu\n      x = tf.nn.relu(x)\nNode: 'model/activation/Relu'\nDNN library is not found.\n\t [[{{node model/activation/Relu}}]] [Op:__inference_train_function_6892]"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "if restart == \"yes\":\n",
    "    print(\"restarting from epoch #\", restart_epoch)\n",
    "    model.load_weights(model_name + '.' + str(restart_epoch-1).zfill(3) +'.h5')\n",
    "    \n",
    "for epoch in range(restart_epoch, epochs):\n",
    "    print(\"=\" * 80)\n",
    "    print(epoch, \"/\", epochs, \" EPOCHS\")\n",
    "    acc = []\n",
    "\n",
    "    for X_train_bulk, Y_train_bulk in get_bulk(X_train, Y_train, images_per_bulk):\n",
    "        for X_batch, Y_batch in get_batch(X_train_bulk, Y_train_bulk, images_per_batch):\n",
    "            print('Training start for a batch')\n",
    "            # normalize data\n",
    "            X_batch = X_batch.astype('float32')\n",
    "            X_batch /= 255\n",
    "            Y_batch = Y_batch.astype('float32')\n",
    "            Y_batch /= 255\n",
    "            model.train_on_batch(X_batch, Y_batch)\n",
    "            score = model.evaluate(X_batch, Y_batch)\n",
    "            print(\"batch accuracy:\", score)\n",
    "            acc.append(score)#\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    print(\"Train accuracy (mean)\", np.mean(acc))\n",
    "    print(\"Train accuracy (max)\", np.max(acc))\n",
    "    model.save(model_name + '.' + str(epoch).zfill(3) +'.h5')\n",
    "\n",
    "    #with tf.device('/cpu:0'):\n",
    "    #    loss, acc = model.evaluate(X_test, Y_test, verbose=2)\n",
    "    #    print(\"Loss and accuracy\", loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([np.array(Image.open(file)) for file in x_test[0:testimages_max]])\n",
    "Y_test = np.array([np.array(Image.open(file)) for file in y_test[0:testimages_max]])\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "Y_test = Y_test.astype('float32')\n",
    "Y_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition to show original image and reconstructed image\n",
    "def showOrigDec(orig, noise, denoise, num=5):\n",
    "    import matplotlib.pyplot as plt\n",
    "    n = num\n",
    "    plt.figure()  #figsize=(20, 6))\n",
    "\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(5, n, i+1)\n",
    "        _orig = np.copy(orig[i])\n",
    "        _orig *= 255\n",
    "        _orig = _orig.astype('uint8')\n",
    "        plt.imshow(_orig.reshape(window_height, window_width, 3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display noisy image\n",
    "        ax = plt.subplot(5, n, i +1 + n)\n",
    "        _noise = np.copy(noise[i])\n",
    "        _noise *= 255\n",
    "        _noise = _noise.astype('uint8')\n",
    "        plt.imshow(_noise.reshape(window_height, window_width, 3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display diff between the original and the noisy image\n",
    "        ax = plt.subplot(5, n, i +1 + n + n)\n",
    "        im_diff = _noise - _orig\n",
    "        im_diff_center = np.floor_divide(im_diff, 2) + 128      \n",
    "        im_diff_center = im_diff_center.astype('uint8')        \n",
    "        plt.imshow(im_diff_center.reshape(window_height, window_width, 3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display the denoised image\n",
    "        ax = plt.subplot(5, n, i +1 + n + n + n)\n",
    "        _denoise = np.copy(denoise[i])\n",
    "        _denoise *= 255\n",
    "        _denoise = _denoise.astype('uint8')\n",
    "        plt.imshow(_denoise.reshape(window_height, window_width, 3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display diff between the original and the denoised image\n",
    "        ax = plt.subplot(5, n, i +1 + n + n + n + n)\n",
    "        _denoise = np.copy(denoise[i])\n",
    "        _denoise *= 255\n",
    "        _orig = np.copy(orig[i])\n",
    "        _orig *= 255\n",
    "        im_diff = _denoise - _orig\n",
    "        im_diff_center = np.floor_divide(im_diff, 2) + 128      \n",
    "        im_diff_center = im_diff_center.astype('uint8')   \n",
    "        plt.imshow(im_diff_center.reshape(window_height, window_width, 3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('scannoise_remover.' + str(testimage_epoch).zfill(3) + '.h5')\n",
    "model.summary()\n",
    "print(\"testimages: \", testimages_max)\n",
    "with tf.device('/gpu:1'):\n",
    "    magazine_test = model.predict(X_test[0:testimages_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showOrigDec(X_test, Y_test, magazine_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
